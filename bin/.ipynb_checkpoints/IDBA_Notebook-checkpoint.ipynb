{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env\n",
    "# %who\n",
    "# %who_ls\n",
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, absConfPath = GetBackSomeDirectoryAndGetAbsPath('../config/IDBA_Config(advance).ini')\n",
    "# config = configparser.ConfigParser()\n",
    "# config.read(absConfPath)\n",
    "\n",
    "# # seq_config = pickle.load(open(absModConfPath, \"rb\" ))\n",
    "# # print(seq_config)\n",
    "\n",
    "\n",
    "# # print(in_seq_delimeter)\n",
    "# # print(timesteps)\n",
    "# # print(x_cols)\n",
    "# # print(x_cat_cols)\n",
    "# # print(x_clip_min, x_clip_max)\n",
    "# # print(scale_param_dict)\n",
    "# # print(y_col)\n",
    "# # print(num_y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load DBA__Executor.py\n",
    "import matplotlib \n",
    "matplotlib.use('Agg')\n",
    "#%matplotlib inline\n",
    "\n",
    "import configparser\n",
    "import time, os, pickle, ast\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from DBA_GeneralFunc import GetBackSomeDirectoryAndGetAbsPath, TimeCataloging, CreateKey, LevBasedPrint, AddRecommendation\n",
    "from DBA_BQDataImport import ImportDataFromBQ\n",
    "from DBA_RnnDataPreProcessing import RnnDataPreprocessing\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "#from sklearn import metrics\n",
    "#from sklearn.datasets.samples_generator import make_blobs\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Bot Flag Today is based in static window not sliding, i.e. it can include 1hr data of today or \n",
    "    even 23 hr data of today but not a constant 24 hr data\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Main Function Start\n",
    "    t0 = int(time.time())\n",
    "    print('Execution Start ' + str(t0))\n",
    "\n",
    "    # -----------<<<  Setting constant values that are to be used inside function  >>>----------- #\n",
    "    _, absConfPath = GetBackSomeDirectoryAndGetAbsPath('../config/IDBA_Config(advance).ini')\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(absConfPath)\n",
    "    WhichPipeline = config['DataProcessing']['Setting']\n",
    "    MixTodayBotTrueObs = config['DataProcessing']['KeepIsBotTodayFlaggedObs']\n",
    "    #ModelConfigPath = config['ModelPaths']['ModConfigFile']\n",
    "    #_, absModConfPath = GetBackSomeDirectoryAndGetAbsPath(ModelConfigPath)\n",
    "    EncoderModelPath = config['ModelPaths']['EncoderModelFile']\n",
    "    _, absEncModPath = GetBackSomeDirectoryAndGetAbsPath(EncoderModelPath)\n",
    "    EncOutFilePath = config['OutputPaths']['EncodedFeatureTrainDatasetFile']\n",
    "    _, absEncOutPath = GetBackSomeDirectoryAndGetAbsPath(EncOutFilePath)\n",
    "    EncOutGrpFilePath = config['OutputPaths']['EncodedFeatureWithGrpIdTrainDatasetFile']\n",
    "    _, absEncOutGrpPath = GetBackSomeDirectoryAndGetAbsPath(EncOutGrpFilePath)\n",
    "    ## Code to make use of GPU \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "    LevBasedPrint('Inside \"'+main.__name__+'\" function and configurations for this has been set.',0,1)\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------<<<  xyz  >>>--------------------------------------- #\n",
    "    ## Getting Data From BQ\n",
    "    InputDF = ImportDataFromBQ(config)\n",
    "    t1 = int(time.time())\n",
    "    TimeCataloging(config, 'ImportInput', t1 - t0, First = 'On')\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------<<<  xyz  >>>--------------------------------------- #\n",
    "    ## PreProcessing the Data\n",
    "    train_x, test_x, train_y, test_y = RnnDataPreprocessing(InputDF, config)\n",
    "    t2 = int(time.time())\n",
    "    TimeCataloging(config, 'RnnDataPreProcessing', t2 - t1)\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------<<<  xyz  >>>--------------------------------------- #\n",
    "    if WhichPipeline == 'GlTest':\n",
    "        ## Loading Model\n",
    "        EncoderMod = load_model(absEncModPath)\n",
    "        train_x_encoded = EncoderMod.predict(train_x)\n",
    "    t3 = int(time.time())\n",
    "    TimeCataloging(config, 'DevelopingEncoding', t3 - t2)\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------<<<  xyz  >>>--------------------------------------- #\n",
    "    ## Data Post Processing\n",
    "    FeaturesDF = pd.DataFrame(train_x_encoded)\n",
    "    newName = {}\n",
    "    cols = FeaturesDF.columns.tolist() \n",
    "    newCols = [ 'encVar_' + str(col) for col in cols ]\n",
    "    for i in range(len(cols)):\n",
    "        newName[cols[i]] = newCols[i]\n",
    "    FeaturesDF.rename(columns= newName, inplace=True)\n",
    "\n",
    "    ## Adding Identifier Columns\n",
    "    FeaturesDF['SID'] = InputDF['SID']\n",
    "    FeaturesDF['SeqsBackFromCurrent'] = InputDF['SeqsBackFromCurrent']\n",
    "    FeaturesDF['apidata__zpsbd6'] = InputDF['apidata__zpsbd6']\n",
    "    FeaturesDF['RecentHit_TimeStamp'] = InputDF['RecentHit_TimeStamp']\n",
    "    FeaturesDF['isBotHits'] = InputDF['isBotHits']\n",
    "    FeaturesDF['Hits'] = InputDF['Hits']\n",
    "    FeaturesDF['botFlagTrue_Today'] = InputDF['botFlagTrue_Today']  ### These Observations were not removed earlier.\n",
    "\n",
    "    # ---------------------------------------<<<  xyz  >>>--------------------------------------- #\n",
    "    ## Treating Observations which were flagged as bot today\n",
    "    # print('DataFrame Shape before Treating IsBotTodayFlag:', FeaturesDF.shape)\n",
    "    if MixTodayBotTrueObs == False:\n",
    "        FeaturesDF = FeaturesDF.loc[FeaturesDF['botFlagTrue_Today']!=1, :].reset_index(drop=True)\n",
    "    # else: ## now ignoring the format changing of IDBA.\n",
    "    #     FeaturesDF.loc[FeaturesDF['botFlagTrue_Today'] == 1, 'SeqsBackFromCurrent'] = [ gr.split('_')[0] + '_'+ str(1000+int(gr.split('_')[1])) \n",
    "    #                                                                                    for gr in FeaturesDF.loc[FeaturesDF['botFlagTrue_Today'] == 1, 'SeqsBackFromCurrent'] ] \n",
    "    #     FeaturesDF.loc[FeaturesDF['botFlagTrue_Today'] == 1, 'RecentHit_TimeStamp'] = 0\n",
    "    #     FeaturesDF.loc[FeaturesDF['botFlagTrue_Today'] == 1, 'isBotHits'] = 20\n",
    "    # print('DataFrame Shape after Treating IsBotTodayFlag:', FeaturesDF.shape)\n",
    "\n",
    "    # ---------------------------------------<<<  xyz  >>>--------------------------------------- #\n",
    "    ## Rearranging columns order & dropping 'botFlagTrue_Today'\n",
    "    newCols = ['SID', 'SeqsBackFromCurrent', 'apidata__zpsbd6', 'RecentHit_TimeStamp', 'isBotHits', 'Hits'] + newCols\n",
    "    FeaturesDF = FeaturesDF.reindex(columns=newCols)\n",
    "\n",
    "    LevBasedPrint('Final DataFrame Shape containing the encoded columns : '+str(FeaturesDF.shape), 0)\n",
    "    ##    Data Post Processing END\n",
    "    t4 = int(time.time())\n",
    "    TimeCataloging(config, 'PostEncodingProcessing', t4 - t3)\n",
    "    \n",
    "    \n",
    "    # ---------------------------------------<<<  xyz  >>>--------------------------------------- #\n",
    "    ## Saving the newly generated encoded features to a DF\n",
    "    FeaturesDF.to_csv(absEncOutPath,index=False)\n",
    "    # print(FeaturesDF.columns.tolist())\n",
    "    # display(FeaturesDF.head())\n",
    "\n",
    "    # ---------------------------------------<<<  xyz  >>>--------------------------------------- #\n",
    "    ## Creating Clusters\n",
    "    def GenerateSidWiseCluster(config):\n",
    "        '''\n",
    "        '''\n",
    "        EncDF = train_x_encoded.copy()\n",
    "        DF = FeaturesDF.copy()\n",
    "        SIDs = ast.literal_eval(config['DataProcessing']['SIDs'])\n",
    "\n",
    "        for sid in SIDs:\n",
    "            #X = StandardScaler().fit_transform(train_x_encoded[r['sid'] == sid])\n",
    "            X = EncDF[DF['SID'] == sid]\n",
    "            db = DBSCAN().fit(X)\n",
    "            DF.loc[DF['SID'] == sid, 'cluster_id'] = [ str(sid) + '_grp' + str(ele) for ele in list(db.labels_) ]\n",
    "            print('='*10,sid,'='*10)\n",
    "            print(DF.loc[DF['SID'] == sid,:].groupby(['cluster_id'])['isBotHits'].describe())    \n",
    "            print('-'*25)\n",
    "            print(DF.loc[DF['SID'] == sid,:].groupby(['cluster_id'])['isBotHits'].describe()['mean'].std())        \n",
    "            print('='*25)\n",
    "            print()\n",
    "\n",
    "        return DF\n",
    "\n",
    "    # ---------------------------------------<<<  xyz  >>>--------------------------------------- #\n",
    "    DF = GenerateSidWiseCluster(config)\n",
    "    DF.loc[:,'KEY'] = CreateKey(DF, ['SID', 'SeqsBackFromCurrent', 'apidata__zpsbd6'])\n",
    "    DF.drop(columns=['SID', 'SeqsBackFromCurrent', 'apidata__zpsbd6'], inplace=True)\n",
    "    cols = ['KEY', 'cluster_id'] + [ ele for ele in list(DF.columns) if ele not in ['KEY', 'cluster_id'] ] \n",
    "    DF = DF.reindex(columns = cols)\n",
    "    DF.to_csv(absEncOutGrpPath,index=False)\n",
    "    \n",
    "    #display(DF.head())\n",
    "    \n",
    "    t5 = int(time.time())\n",
    "    TimeCataloging(config, 'ClusterDevelopment', t5 - t4)\n",
    "    \n",
    "    TimeConsumedReport = TimeCataloging(config, 'WholeExecutionTime', t5 - t0)\n",
    "    \n",
    "    LevBasedPrint('Time Taken',0)\n",
    "    LevBasedPrint('|\\t> Importing Data From BQ :'+str(TimeConsumedReport['ImportInput'])+' sec',0)\n",
    "    LevBasedPrint('|\\t> Processing Data For RNN :'+str(TimeConsumedReport['RnnDataPreProcessing'])+' sec',0)\n",
    "    LevBasedPrint('|\\t> Generating Encoding :'+str(TimeConsumedReport['DevelopingEncoding'])+' sec',0)\n",
    "    LevBasedPrint('|\\t> Post Data Processing :'+str(TimeConsumedReport['PostEncodingProcessing'])+' sec',0)\n",
    "    LevBasedPrint('|\\t> Developing Clusters :'+str(TimeConsumedReport['ClusterDevelopment'])+' sec',0)\n",
    "    LevBasedPrint('|\\t',0)\n",
    "    LevBasedPrint('|\\t>> Whole Execution Time :'+str(TimeConsumedReport['WholeExecutionTime'])+' sec',0)\n",
    "\n",
    "    print('completed main '+str(int(time.time())))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "## Other\n",
    "#from datetime import datetime,timedelta\n",
    "#required_instant = datetime.utcnow()- timedelta(hours = 1)\n",
    "#required_instant.strftime('%m%y'),required_instant.strftime('%d'),required_instant.strftime('%H')\n",
    "#print(required_instant)\n",
    "\n",
    "#run hourly:\n",
    "#for given sids, get scores for IPs in past hour exclude IPs which solved captcha in the day from blacklisting\n",
    "#for each sid, blacklist maximum k IPs with score < threshold store the details of IPs blacklisted to a file\n",
    "\n",
    "## Code to execute a file before this\n",
    "#exec(compile(open('ss_lib_1.py', \"rb\").read(), 'ss_lib_1.py', 'exec'))\n",
    "\n",
    "## Code to make use of GPU \n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  ###---------> Understand this\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" # so the IDs match nvidia-smi\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # \"0, 1\" for multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FeaturesDF.loc[[ True if i == 'Seq_-1' else False for i in FeaturesDF['SeqsBackFromCurrent'] ],'RecentHit_TimeStamp'].tolist()[0:10])\n",
    "print(FeaturesDF.loc[[ True if i == 'Seq_0' else False for i in FeaturesDF['SeqsBackFromCurrent'] ],'RecentHit_TimeStamp'].tolist()[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(FeaturesDF))\n",
    "print(len(FeaturesDF.loc[[ True if i == 'Seq_-1' else False for i in FeaturesDF['SeqsBackFromCurrent'] ],:]))\n",
    "print(len(FeaturesDF.loc[[ True if i == 'Seq_0' else False for i in FeaturesDF['SeqsBackFromCurrent'] ],:]))\n",
    "print(len(FeaturesDF.loc[[ True if i == 'Seq_1' else False for i in FeaturesDF['SeqsBackFromCurrent'] ],:]))\n",
    "print(len(FeaturesDF.loc[[ True if i == 'Seq_2' else False for i in FeaturesDF['SeqsBackFromCurrent'] ],:]))\n",
    "print(len(FeaturesDF.loc[[ True if i == 'Seq_3' else False for i in FeaturesDF['SeqsBackFromCurrent'] ],:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OfficeProjVirEnv",
   "language": "python",
   "name": "officeprojvirenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
