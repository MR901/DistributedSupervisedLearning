{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile filename.py\n",
    "# %load executor.py\n",
    "# %load adaptive_algo.py\n",
    "# %load feedback_collector.py\n",
    "# %load UC__executor_clustering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load SL1_ImportData.py\n",
    "\n",
    "# Importing Data\n",
    "'''\n",
    "Description: \n",
    "    This file provide some function that are toe used for importing data .\n",
    "Function this file Contains:\n",
    "    - ImportData: Used to import data either from BQ or from Storage.\n",
    "'''\n",
    "\n",
    "# ----------------------------------------------- Loading Libraries ----------------------------------------------- #\n",
    "import pandas as pd\n",
    "import glob, os, ast, time\n",
    "from datetime import datetime, date, timedelta\n",
    "from SL0_GeneralFunc import LevBasedPrint, AddRecommendation\n",
    "\n",
    "\n",
    "# ------------------------------------------ GrabAnySizeDatafromGoogleBQ ------------------------------------------ #\n",
    "def Exec_BQ(query, projectid):\n",
    "    LevBasedPrint('Inside \"'+Exec_BQ.__name__+'\" function.',3,1)\n",
    "    LevBasedPrint('',3,1)\n",
    "    return pd.io.gbq.read_gbq(query, project_id=projectid, index_col=None, col_order=None, reauth=False, private_key=None) #, verbose=True deprecated\n",
    "\n",
    "\n",
    "def GenerateTableNames(config):\n",
    "    '''\n",
    "    Make use of Domain based parameters to get the data.\n",
    "    '''\n",
    "    # -----------<<<  Setting constant values that are to be used inside function  >>>----------- #\n",
    "    DatasetName = 'ss-production-storage'\n",
    "    SIDs = ast.literal_eval(config['DomainConfig']['SIDs'])\n",
    "    DataGrabMethodology = config['DomainConfig']['UseStaticOrDynamicCurrentDay']\n",
    "    LevBasedPrint('Inside \"'+GenerateTableNames.__name__+'\" function and configurations for this has been set.',3,1)\n",
    "    LevBasedPrint('Data collection methodology that has been selected : ' + str(DataGrabMethodology),3)\n",
    "    if DataGrabMethodology == 'static':\n",
    "        Dates = ast.literal_eval(config['IfStatic']['Date']) \n",
    "        StaDataWindow = ast.literal_eval(config['IfStatic']['DataGrabWindow_Days'])\n",
    "    elif DataGrabMethodology == 'dynamic':\n",
    "        DynDataWindow = int(ast.literal_eval(config['IfDynamic']['DataGrabWindow_Hr']))\n",
    "    else:\n",
    "        txt = 'Exception: Wrong Configuration has been passed in \"UseStaticOrDynamicCurrentDay\".'\n",
    "        AddRecommendation(txt, config)\n",
    "        raise Exception(txt)\n",
    "    \n",
    "    # -----------------------------<<<  Generating Table Names  >>>------------------------------ #\n",
    "    ## Generating Table Names\n",
    "    if DataGrabMethodology == 'static':\n",
    "        if StaDataWindow != '-':\n",
    "            CustomDate = date(2000 + int(Dates[0][4:6]), int(Dates[0][2:4]), int(Dates[0][0:2])) \n",
    "            format = '%d%m%y'\n",
    "            Dates = [ (CustomDate + timedelta(days=i)).strftime(format) for i in range(int(StaDataWindow)) ]\n",
    "        TableToInclude = ''\n",
    "        for i in range(len(SIDs)):\n",
    "            for j in range(len(Dates)):\n",
    "                TableToInclude += '\\n\\tTABLE_QUERY([{}.Citadel_Stream],\\'table_id like \"'.format(DatasetName) + SIDs[i] + '_' + Dates[j] + '_%\"\\'),'\n",
    "    elif DataGrabMethodology == 'dynamic':\n",
    "        CurrentTime = datetime(time.gmtime().tm_year, time.gmtime().tm_mon, time.gmtime().tm_mday, time.gmtime().tm_hour, time.gmtime().tm_min, time.gmtime().tm_sec) ## UTC        \n",
    "        TableDateToTake = []\n",
    "        while DynDataWindow >= -1:  ## -1 to even include the current hour table\n",
    "            tempDate = CurrentTime - timedelta(days = 0, hours = DynDataWindow, minutes = 0)\n",
    "            TableDateToTake.append(tempDate.strftime(format = '%d%m%y_%H'))\n",
    "            DynDataWindow -= 1\n",
    "        TableToInclude, TableCnt = '', 0\n",
    "        for i in range(len(SIDs)):\n",
    "            for j in range(len(TableDateToTake)):\n",
    "                TableCnt += 0\n",
    "                TableToInclude += '\\n\\tTABLE_QUERY([{}.Citadel_Stream],\\'table_id like \"'.format(DatasetName) + SIDs[i] + '_' + TableDateToTake[j] + '%\"\\'),'\n",
    "        LevBasedPrint('Total number of tables accessed : '+str(TableCnt),3)\n",
    "    # ---------------------------------------<<<  xyz  >>>--------------------------------------- #\n",
    "    LevBasedPrint('',3,1)\n",
    "    return TableToInclude\n",
    "    # ------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "def GrabAnySizeDatafromGoogleBQ(config):\n",
    "    '''\n",
    "    Incase if dataset size is too large then this function will enable the extraction of whole dataset by getting the data in chunks\n",
    "    '''\n",
    "    # -----------<<<  Setting constant values that are to be used inside function  >>>----------- #\n",
    "    ModuleSetting = config['Config']['ModuleSettingRuleName']\n",
    "    BQ_Cred = config['BigQueryConfig']['ProjectID']\n",
    "    if ModuleSetting == 'ICLSSTA': BinSizeBasedOnPeriod_Hr = int(config['Config']['ICLSSTA_BinSizeBasedOnPeriod_Hr'])\n",
    "    BQ_QueryFile = config['InputPaths']['BQ_DataImportQuery']\n",
    "    LimitToStartWith = config['BigQueryConfig']['BQ_LimitToStart']\n",
    "    LimitDecreaseFactor = float(config['BigQueryConfig']['BQ_LimitDecreaseFactor'])\n",
    "    LevBasedPrint('Inside \"'+GrabAnySizeDatafromGoogleBQ.__name__+'\" function and configurations for this has been set.',2,1)\n",
    "    \n",
    "    # -------------------------<<<  Generating Tables Name To Query  >>>------------------------- #\n",
    "    TableToInclude = GenerateTableNames(config)\n",
    "    #print(TableToInclude)\n",
    "    \n",
    "    # -------------------------<<<  Creating Bin Setting For ICLSSTA  >>>------------------------ #\n",
    "    ## Getting the string that will be used to create bins for grouping based on a certain TimePeriod\n",
    "    GroupsToInclude = ''\n",
    "    if ModuleSetting == 'ICLSSTA':\n",
    "        for i in range(1000): ##even if the bin size is as small as an hour, BQ has a limitation of accessing upto a max of 1000 Table, so this is the max possible limit \n",
    "            ll_insec = int(i*BinSizeBasedOnPeriod_Hr *3600)\n",
    "            ul_insec = int((i+1)*BinSizeBasedOnPeriod_Hr *3600 - 1)\n",
    "            GroupsToInclude += '\\n\\tWHEN (CurrentTimeStamp - CurrentHitTimeStamp) BETWEEN {low} AND {upp} THEN \"Bin_{WhichBin}\"'.format(low= ll_insec,upp= ul_insec, WhichBin= i)\n",
    "    \n",
    "    # ------------------------<<<  Reading Query From External File  >>>------------------------- #\n",
    "    LevBasedPrint('Read from a locally saved Query File', 2)\n",
    "    queryfile = open(BQ_QueryFile, 'r')\n",
    "    query = queryfile.read()\n",
    "    queryfile.close()\n",
    "    \n",
    "    # --------------------<<<  Importing Data in Max possible batch size  >>>-------------------- #\n",
    "    ## looping over the limit and offset to grab the maximum possible bite in terms of observation that can be gathered\n",
    "    ## GP\n",
    "    start = int(LimitToStartWith)  # should be equal to the maximum number of observation that you want to extract\n",
    "    ratio = 1/LimitDecreaseFactor\n",
    "    limit = 1000  ## util which pt to try to gather the data ## Hardcoded\n",
    "    length = 1000\n",
    "    # query='''SELECT 1 limit {lim} offset {off}'''\n",
    "    \n",
    "    DF = pd.DataFrame()\n",
    "    ##GP\n",
    "    for i in [ int(start * ratio ** (n - 1)) for n in range(1, length + 1) if start * ratio ** (n - 1) > limit ]:\n",
    "        if DF.shape == (0, 0):\n",
    "            try:\n",
    "                offcurr = 0\n",
    "                while offcurr < start:\n",
    "                    LevBasedPrint('Setting used in extracting data from BQ:\\tNo. of obs. extracted per cycle (limit) = ' + str(i) + '\\tOffset = ' + str(offcurr),2)\n",
    "                    QueryToUse = query.format(BinToUse = GroupsToInclude, TableToInclude = TableToInclude, lim = str(i), off = str(offcurr))\n",
    "                    tempDF = Exec_BQ(QueryToUse, BQ_Cred)\n",
    "                    DF = DF.append(tempDF, ignore_index = True)\n",
    "                    offcurr += i\n",
    "\n",
    "            except Exception as error:\n",
    "                txt = 'Exception: In importing data from BQ was thrown!\\nLimit used: ' + str(i) + '\\n' + str(error)\n",
    "                LevBasedPrint(txt, 2)\n",
    "                AddRecommendation(txt, config)\n",
    "                # raise Exception(txt)\n",
    "    \n",
    "    # ---------------------------------------<<<  xyz  >>>--------------------------------------- #\n",
    "    LevBasedPrint('',2,1)\n",
    "    return DF\n",
    "    # ------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# -------------------------------------------------- ImportData --------------------------------------------------- #\n",
    "def ImportData(config):\n",
    "    \"\"\"\n",
    "    Can be used to import data from either storage or BQ\n",
    "    \n",
    "    \n",
    "    Extracts any size data from any SID of any number of days.\n",
    "    \n",
    "    Works in Two Configuration(config['aim']['Task']), namely 'TrainTest' & 'GlTest'\n",
    "    'TrainTest' is for models training purpose where This Dataset is split later too make dataset size adequate for training uing sampling\n",
    "    'GlTest' is purely for prediction purpose, i.e. it will be used as testset only and will consume saved model to provide labels to observations\n",
    "    \"\"\"\n",
    "    # -----------<<<  Setting constant values that are to be used inside function  >>>----------- #\n",
    "    AccessDataFrom = config['DataCollection']['GetDataFrom']\n",
    "    if AccessDataFrom == 'BQ':\n",
    "        SettingToUse = config['IterationAim']['Task']\n",
    "        if SettingToUse: GlTestDataSize = int(config['IterationAim']['GlTest_DataGrabWindow_Hr'])\n",
    "        FileLocalSavingName = config['InputPaths']['BQ_RawDataStoringName'].format(SettingToUse)\n",
    "        GetNewCopy = config['DomainConfig']['BQ_GetNewCopyOfData']\n",
    "    elif AccessDataFrom ==  'Storage':\n",
    "        FileName = config['InputPaths']['Storage_RawData']\n",
    "    else:\n",
    "        print('Wrong setting in \"GetDataFrom\", current value is {}'.format(AccessDataFrom))\n",
    "        txt = 'Exception: Wrong Configuration has been passed in \"GetDataFrom\".'\n",
    "        AddRecommendation(txt, config)\n",
    "        raise Exception(txt)\n",
    "    LevBasedPrint('Inside \"'+ImportData.__name__+'\" function and configurations for this has been set.',1,1)\n",
    "    \n",
    "    \n",
    "    LevBasedPrint('Accessing data from {}'.format(AccessDataFrom), 1)\n",
    "    # ----------------------------<<<  Accessing Data from BQ  >>>------------------------------- #\n",
    "    if AccessDataFrom == 'BQ':\n",
    "        \n",
    "        # -----------------------<<<  Setting Configuration for GlTest  >>>-------------------------- #\n",
    "        if(SettingToUse == 'GlTest'):\n",
    "            config['IfStatic']['DataGrabWindow_Days'] = str(int(GlTestDataSize/24 + 1))\n",
    "            config['IfDynamic']['DataGrabWindow_Hr'] = str(GlTestDataSize + 1)\n",
    "\n",
    "        # --------------------------<<<  Get New Copy Of Data Or Reuse  >>>-------------------------- #\n",
    "        if (os.path.exists(FileLocalSavingName) == False) | (GetNewCopy in ['True', 'true', 'T', 't', 'Yes', 'yes', 'Y', 'y']):\n",
    "            DF = GrabAnySizeDatafromGoogleBQ(config)\n",
    "            # if(SettingToUse == 'GlTest'):\n",
    "            #     DF.drop(DF[DF.BinsBackFromCurrent != 'Bin_0'].index, inplace=True)\n",
    "            #     DF.reset_index(drop=True, inplace=True)\n",
    "            DF.to_csv(FileLocalSavingName, index=False)#, sep='|', encoding='utf-8')\n",
    "            LevBasedPrint('Data extracted from BQ and saved locally to the File: '+ FileLocalSavingName, 1)\n",
    "        else:\n",
    "            DF = pd.read_csv(FileLocalSavingName)#, sep='|', encoding='utf-8')\n",
    "            LevBasedPrint('Data Loaded From the File: '+ FileLocalSavingName, 1)\n",
    "        LevBasedPrint('Data Shape: '+str(DF.shape), 1 )\n",
    "    # --------------------------<<<  Accessing Data from Storage  >>>---------------------------- #\n",
    "    elif AccessDataFrom == 'Storage':\n",
    "        DF = pd.read_csv(FileName)#, sep='|', encoding='utf-8')\n",
    "        LevBasedPrint('Data Loaded From the File: '+ FileName, 1)\n",
    "    \n",
    "    # ---------------------------------------<<<  xyz  >>>--------------------------------------- #\n",
    "    LevBasedPrint('',1,1)\n",
    "    return DF\n",
    "    # ------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------- #\n",
    "## AP\n",
    "# start = int(LimitToStartWith)  # should be equal to the maximum number of observation that you want to extract\n",
    "# stop = -1\n",
    "# step = -int(start/LimitDecreaseFactor)\n",
    "# limit = int(start/LimitDecreaseFactor)  ## util which pt to try to gather the data\n",
    "##AP\n",
    "# for i in [i for i in range(start,stop, step) if i >= limit]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Start 1543927626\n",
      "\t+--------------------------------------------------------------------------------------------------------\n",
      "\t| Inside \"ImportData\" function and configurations for this has been set.\n",
      "\t| Accessing data from Storage\n",
      "\t| Data Loaded From the File: ../data/InputData/RawExtractedData_TrainTest.csv\n",
      "\t+--------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SID</th>\n",
       "      <th>BinsBackFromCurrent</th>\n",
       "      <th>apidata__zpsbd6</th>\n",
       "      <th>RecentHit_TimeStamp</th>\n",
       "      <th>isBotHits</th>\n",
       "      <th>Hits</th>\n",
       "      <th>D_UzmaToD_UA</th>\n",
       "      <th>HitsToD_Uzmc</th>\n",
       "      <th>D_PageVisitedToHits</th>\n",
       "      <th>PageActToD_PageVisit</th>\n",
       "      <th>BrowsrActToD_BrowsrUsed</th>\n",
       "      <th>AvgMedianTimeDiffBWHits</th>\n",
       "      <th>AvgAvgTimeDiffBWHits</th>\n",
       "      <th>StandDeviatAvgTimeDiffBWHits</th>\n",
       "      <th>AvgHitsPerUnitTime</th>\n",
       "      <th>DiffOfAvgTimeDiffBWHitsWhnGrpIPAndIPUzma</th>\n",
       "      <th>ZScoreAvgAvgTimeDiffBWHits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3641</td>\n",
       "      <td>Bin_1</td>\n",
       "      <td>86.176.164.90</td>\n",
       "      <td>1543881689</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.485281</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.682213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3641</td>\n",
       "      <td>Bin_1</td>\n",
       "      <td>107.77.210.218</td>\n",
       "      <td>1543881738</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.437500</td>\n",
       "      <td>5.560276</td>\n",
       "      <td>0.901388</td>\n",
       "      <td>3.312500</td>\n",
       "      <td>6.395656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3641</td>\n",
       "      <td>Bin_1</td>\n",
       "      <td>96.10.138.178</td>\n",
       "      <td>1543881734</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.033333</td>\n",
       "      <td>17.306068</td>\n",
       "      <td>0.993358</td>\n",
       "      <td>7.966667</td>\n",
       "      <td>5.302515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3641</td>\n",
       "      <td>Bin_1</td>\n",
       "      <td>99.203.14.40</td>\n",
       "      <td>1543881687</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.125000</td>\n",
       "      <td>12.041595</td>\n",
       "      <td>0.780473</td>\n",
       "      <td>6.375000</td>\n",
       "      <td>4.431772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3641</td>\n",
       "      <td>Bin_1</td>\n",
       "      <td>142.177.187.158</td>\n",
       "      <td>1543881630</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.50</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>40.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>14.849242</td>\n",
       "      <td>0.380945</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>3.191030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SID BinsBackFromCurrent  apidata__zpsbd6  RecentHit_TimeStamp  isBotHits  \\\n",
       "0  3641               Bin_1    86.176.164.90           1543881689          0   \n",
       "1  3641               Bin_1   107.77.210.218           1543881738          0   \n",
       "2  3641               Bin_1    96.10.138.178           1543881734          0   \n",
       "3  3641               Bin_1     99.203.14.40           1543881687          0   \n",
       "4  3641               Bin_1  142.177.187.158           1543881630          0   \n",
       "\n",
       "   Hits  D_UzmaToD_UA  HitsToD_Uzmc  D_PageVisitedToHits  \\\n",
       "0     2           1.0      1.000000                 1.00   \n",
       "1     4           1.0      1.000000                 0.75   \n",
       "2     5           1.0      1.000000                 1.00   \n",
       "3     4           1.0      1.333333                 0.75   \n",
       "4     2           1.0      1.000000                 1.50   \n",
       "\n",
       "   PageActToD_PageVisit  BrowsrActToD_BrowsrUsed  AvgMedianTimeDiffBWHits  \\\n",
       "0              1.000000                      8.0                      6.0   \n",
       "1              5.333333                     60.0                      9.0   \n",
       "2              1.600000                     80.0                      5.0   \n",
       "3              1.333333                      4.0                      9.0   \n",
       "4              4.666667                     40.0                     10.0   \n",
       "\n",
       "   AvgAvgTimeDiffBWHits  StandDeviatAvgTimeDiffBWHits  AvgHitsPerUnitTime  \\\n",
       "0              3.000000                      8.485281            0.666644   \n",
       "1              4.437500                      5.560276            0.901388   \n",
       "2              5.033333                     17.306068            0.993358   \n",
       "3              5.125000                     12.041595            0.780473   \n",
       "4              5.250000                     14.849242            0.380945   \n",
       "\n",
       "   DiffOfAvgTimeDiffBWHitsWhnGrpIPAndIPUzma  ZScoreAvgAvgTimeDiffBWHits  \n",
       "0                                  3.000000                    7.682213  \n",
       "1                                  3.312500                    6.395656  \n",
       "2                                  7.966667                    5.302515  \n",
       "3                                  6.375000                    4.431772  \n",
       "4                                  5.250000                    3.191030  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import os, ast, time\n",
    "from SL0_GeneralFunc import GetBackSomeDirectoryAndGetAbsPath, TimeCataloging, CreateKey, LevBasedPrint, AddRecommendation\n",
    "\n",
    "StartTime = int(time.time())\n",
    "print('Execution Start ' + str(StartTime))\n",
    "\n",
    "ConfigFilePath = '../config/ISLSSTA_Config.ini'\n",
    "_, absModConfPath = GetBackSomeDirectoryAndGetAbsPath(ConfigFilePath)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(absModConfPath)\n",
    "\n",
    "# config_clust['aim']['Task'] = 'GlTest'    ################################ Using This To Change The Configuration\n",
    "\n",
    "input_raw_df = ImportData(config)\n",
    "input_raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from UCorCS_DailySIDTrafficStatus import UnderstandEnvironmentData\n",
    "\n",
    "if config_clust['TriggerTheseFunctions']['UnderstandEnvironmentData'] != 'False': ## To Run Code Below Or Not\n",
    "    UnderstandEnvironmentData(config_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i for i in config_clust['MovingOutputFile']['DimClustAlgoPair'].split(\"'\") if len(i) > 3])\n",
    "print([ i for i in config_clust['DataProcessing_General']['FeatureToIgnore'].split(\"'\") if len(i) > 2 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_clust['aim']['Task'] = 'GlTest'\n",
    "# ImportData_1(config_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_raw_df = ImportData_1(config_clust)\n",
    "input_raw_df.head()\n",
    "# SettingToUse = config_clust['aim']['Task']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SettingToUse = config_clust['aim']['Task']\n",
    "if(SettingToUse == 'TrainTest'):\n",
    "    FileLocalSavingName = config_clust['input']['dataset_dir'] + config_clust['input']['RawDataStorName_TrainTest']\n",
    "elif(SettingToUse == 'GlTest'):\n",
    "    FileLocalSavingName = config_clust['input']['dataset_dir'] + config_clust['input']['RawDataStorName_TrainTest']\n",
    "    #FileLocalSavingName = config_clust['input']['dataset_dir'] + config_clust['input']['RawDataStorName_GlTest']\n",
    "\n",
    "input_raw_df = pd.read_csv(FileLocalSavingName, sep = '|', encoding=\"utf-8\")\n",
    "print(input_raw_df.shape)\n",
    "input_raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_clust['aim']['PaceMode'] == 'Off':\n",
    "    temp = pd.DataFrame(input_raw_df.isnull().sum(), columns = ['IsNullSum'])\n",
    "    temp['dtypes'] = input_raw_df.dtypes.tolist()\n",
    "    temp['IsNaSum'] = input_raw_df.isna().sum().tolist()\n",
    "    temp = temp.join(input_raw_df.describe().T).fillna('')\n",
    "    display(temp)\n",
    "    print('\\nMax isBotHits :', input_raw_df['isBotHits'].max())\n",
    "\n",
    "    # np.isinf(input_raw_df[[ i for i in AllFeature if i not in FeatureToIgnore ]]).any()\n",
    "    # np.isnan(yy).any()\n",
    "\n",
    "    # np.isinf(xx).any()\n",
    "    # np.isinf(yy).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UC_DataProcessing_Executor import DataPreProcess_1\n",
    "from UC_DataExploration import DataExploration_1\n",
    "from UC_DataProcessing_GenMiniFunc import GenerateCorrelationPlot\n",
    "\n",
    "# TrainDF, TestDF, OutlierDF \n",
    "train_processed_raw_df, test_processed_raw_df, outlier_df = DataPreProcess_1(input_raw_df, config_clust)\n",
    "\n",
    "if config_clust['TriggerTheseFunctions']['DataExploration'] != 'False': \n",
    "    print('Initiating Data Exploration Mode')\n",
    "    DataExploration_1(input_raw_df, config_clust)\n",
    "    #DataExploration_1(TrainDF, config_clust)\n",
    "\n",
    "if config_clust['TriggerTheseFunctions']['GenerateCorrelationPlot'] != 'False':\n",
    "    GenerateCorrelationPlot(train_processed_raw_df, config_clust)\n",
    "    GenerateCorrelationPlot(outlier_df, config_clust)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_clust['aim']['PaceMode'] == 'Off':\n",
    "    for i in [TrainDF, TestDF, OutlierDF]:\n",
    "    #     print()\n",
    "        df = i.copy()\n",
    "        temp = pd.DataFrame(df.isnull().sum(), columns = ['IsNullSum'])\n",
    "        temp['dtypes'] = df.dtypes.tolist()\n",
    "        temp['IsNaSum'] = df.isna().sum().tolist()\n",
    "        temp = temp.join(df.describe().T).fillna('')\n",
    "        display(temp)\n",
    "        print('\\nMax isBotHits :', input_raw_df['isBotHits'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation and Dimension Transformation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Dimension Transformation on Orginal Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from UC_DataDimensionProcessing import DimensionReduction_1\n",
    "\n",
    "## Removing Previous Iteration Files\n",
    "PreviousIterationFiles = glob.glob(config_clust['MovingOutputFile']['DirToMoveFrom'] + ('DataDimensionTransformation_' + '*.{FileType}').format(FileType='csv'))\n",
    "[ os.unlink(path) for path in PreviousIterationFiles ]\n",
    "\n",
    "# DimRedClustAlgoDict = ast.literal_eval('''{ ('LDA', 'LDA_param1') : [('DBSCAN', 'DBSCAN_param_1')] , ('ICA', 'ICA_param1') : [('IsolationForest', 'IsolationForest_param_1')] , ('PCA', 'PCA_param1') : [('KMeans', 'KMeans_param_1')] }''')\n",
    "DimRedClustAlgoDict = ast.literal_eval(config_clust['AnomalyClusterConfiguration']['DataTransfRedNClustAlgo'])\n",
    "for DimRed in DimRedClustAlgoDict.keys():\n",
    "    print('Data Dimension transformation Algo Used : ', DimRed[0], '\\t\\tWith Params : ', DimRed[1])\n",
    "    train_dimen_transf_df, test_dimen_transf_df = DimensionReduction_1(train_processed_raw_df, test_processed_raw_df, DimRed[0], DimRed[1], config_clust) \n",
    "    # display(train_dimen_transf_df.head())\n",
    "    # display(test_dimen_transf_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_processed_raw_df['isBotHits'].max())\n",
    "# print(train_dimen_transf_df['isBotHits'].max())\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UC_DataClustering import ClusteringApplied_1\n",
    "\n",
    "## Removing Previous Iteration Files\n",
    "PreviousIterationFiles = glob.glob(config_clust['MovingOutputFile']['DirToMoveFrom'] + ('*ModelData_' + '*.{FileType}').format(FileType='csv'))\n",
    "[ os.unlink(path) for path in PreviousIterationFiles ]\n",
    "\n",
    "DimRedClustAlgoDict = ast.literal_eval(config_clust['AnomalyClusterConfiguration']['DataTransfRedNClustAlgo'])\n",
    "# DimRedClustAlgoDict = ast.literal_eval('''{ ('LDA', 'LDA_param1') : [('DBSCAN', 'DBSCAN_param_1')] , ('ICA', 'ICA_param1') : [('IsolationForest', 'IsolationForest_param_1')] , ('PCA', 'PCA_param1') : [('KMeans', 'KMeans_param_1')] }''')\n",
    "\n",
    "for DimRed in DimRedClustAlgoDict.keys():\n",
    "    print('Data Dimension transformation Algo Used : ', DimRed[0], '\\t\\tWith Params : ', DimRed[1])\n",
    "    try:\n",
    "        train_dimen_transf_df = pd.read_csv((config_clust['input']['dataset_dir'] + 'DataDimensionTransformation_Train__' + DimRed[0] + '_With_'+ DimRed[1] + '.csv'), sep = '|', encoding=\"utf-8\")\n",
    "    except:\n",
    "        train_dimen_transf_df = None\n",
    "    try:\n",
    "        test_dimen_transf_df = pd.read_csv((config_clust['input']['dataset_dir'] + 'DataDimensionTransformation_Test__' + DimRed[0] + '_With_'+ DimRed[1] + '.csv'), sep = '|', encoding=\"utf-8\")\n",
    "    except:\n",
    "        test_dimen_transf_df = None\n",
    "    for ClustAlgo in DimRedClustAlgoDict[DimRed]:\n",
    "        AlgoCombination = {'DimensionTransformation' : (DimRed[0], DimRed[1]), \n",
    "                           'AnomalyClustering': (ClustAlgo[0], ClustAlgo[1])}\n",
    "        print('|\\t\\tData Segmentation Algo Used : ', ClustAlgo[0], '\\t\\tWith Params : ', ClustAlgo[1])\n",
    "        TrainDF, TestDF = ClusteringApplied_1(train_dimen_transf_df, test_dimen_transf_df, outlier_df, AlgoCombination, config_clust)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UC_DataClustering_NonInliner import CreateAdditionalClusters\n",
    "CreateAdditionalClusters(outlier_df, config_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Move Train Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UC_OutputTransformer_CombinerMover import MoveFileToAdaptDir\n",
    "\n",
    "MoveFileToAdaptDir(config_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Cluster Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from UC_ClusterEvaluation import ClustersEvaluation\n",
    "\n",
    "if config_clust['TriggerTheseFunctions']['ClustersEvaluation'] != 'False':\n",
    "    # ClustersEvaluation(config_clust, 'SingleFile', None, ('data/keyModelsData/ClusterModelData_TrainTest_PCA_KMeans.csv', 'path'))\n",
    "    df = ClustersEvaluation(config_clust, 'MultipleFiles', None, (None,None))\n",
    "    df.set_index(['Algorithm'])\n",
    "    # df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling the Cluster Results Evaluating is Also done in this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UC_OutputTransformer import OutputTransformer\n",
    "Output_Keysets_Df, EnsembleEval_DF = OutputTransformer(config_clust)\n",
    "\n",
    "display(Output_Keysets_Df.head())\n",
    "display(EnsembleEval_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EndTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EndTime - StartTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Start -- Anomaly autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_clust\n",
    "df = input_raw_df.copy()\n",
    "# FilterAnomalyCases(WholeDF.iloc[0:3,], config)\n",
    "\n",
    "## Outliers to be removed from the features\n",
    "AllFeature = [ i for i in config['DataProcessing_General']['AllFeaturesToUtilize'].split(\"'\") if len(i) > 2 ]\n",
    "FeatureToIgnore = [ i for i in config['DataProcessing_General']['FeatureToIgnore'].split(\"'\") if len(i) > 2 ]\n",
    "ColToAnalysis = [ i for i in AllFeature if i not in FeatureToIgnore ]\n",
    "\n",
    "# display(df.head(10))  ## Original Dataset\n",
    "\n",
    "## Copying the feature to a new DF which are to be ignored in dimension tranformation\n",
    "df_transformed = df[FeatureToIgnore].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(training_set, dtype = 'int')\n",
    "\n",
    "# Converting the data into Torch tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)\n",
    "\n",
    "# Creating the architecture of the Neural Network\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NN using Numpy\n",
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NN Using Pytorch\n",
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoders\n",
    "\n",
    "\n",
    "\n",
    "# # Importing the dataset\n",
    "# movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "# users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "# ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "\n",
    "# # Preparing the training set and the test set\n",
    "# training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t')\n",
    "# training_set = np.array(training_set, dtype = 'int')\n",
    "# test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t')\n",
    "# test_set = np.array(test_set, dtype = 'int')\n",
    "\n",
    "# # Getting the number of users and movies\n",
    "# nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "# nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))\n",
    "\n",
    "# # Converting the data into an array with users in lines and movies in columns\n",
    "# def convert(data):\n",
    "#     new_data = []\n",
    "#     for id_users in range(1, nb_users + 1):\n",
    "#         id_movies = data[:,1][data[:,0] == id_users]\n",
    "#         id_ratings = data[:,2][data[:,0] == id_users]\n",
    "#         ratings = np.zeros(nb_movies)\n",
    "#         ratings[id_movies - 1] = id_ratings\n",
    "#         new_data.append(list(ratings))\n",
    "#     return new_data\n",
    "# training_set = convert(training_set)\n",
    "# test_set = convert(test_set)\n",
    "\n",
    "# # Converting the data into Torch tensors\n",
    "# training_set = torch.FloatTensor(training_set)\n",
    "# test_set = torch.FloatTensor(test_set)\n",
    "\n",
    "# Creating the architecture of the Neural Network\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)\n",
    "\n",
    "# Training the SAE\n",
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))\n",
    "\n",
    "# Testing the SAE\n",
    "test_loss = 0\n",
    "s = 0.\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user])\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "        s += 1.\n",
    "print('test loss: '+str(test_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing ---- END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustering.cluster import cluster_visualizer;\n",
    "from pyclustering.cluster.cure import cure;\n",
    "\n",
    "from pyclustering.utils import read_sample;\n",
    "\n",
    "from pyclustering.samples.definitions import FCPS_SAMPLES;\n",
    "\n",
    "# Input data in following format [ [0.1, 0.5], [0.3, 0.1], ... ].\n",
    "input_data = read_sample(FCPS_SAMPLES.SAMPLE_LSUN);\n",
    "\n",
    "# Allocate three clusters:\n",
    "cure_instance = cure(input_data, 3);\n",
    "cure_instance.process();\n",
    "clusters = cure_instance.get_clusters();\n",
    "\n",
    "# Visualize clusters:\n",
    "visualizer = cluster_visualizer();\n",
    "visualizer.append_clusters(clusters, None);\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.handlers\n",
    "logger = None\n",
    "\n",
    "logging_conf = config_reader['logging']\n",
    "config['log_file'] = logging_conf['log_file']\n",
    "log_level_dict = {\n",
    "    'CRITICAL' : logging.CRITICAL,\n",
    "    'ERROR' : logging.ERROR,\n",
    "    'WARNING' : logging.WARNING,\n",
    "    'INFO' : logging.INFO,\n",
    "    'DEBUG' : logging.DEBUG\n",
    "}\n",
    "config['log_level'] = log_level_dict[logging_conf['log_level']]\n",
    "config['log_maxBytes'] = logging_conf.getint('maxBytes')\n",
    "config['log_backupCount'] = logging_conf.getint('backupCount')\n",
    "\n",
    "\n",
    "except Exception as ex:\n",
    "        logger.error('Exception in making API call to {}: {}'.format(path,ex))\n",
    "        exit_with_error()\n",
    "        \n",
    "logger.info('response to add rule api call: {}'.format(res.json()))\n",
    "\n",
    "\n",
    "global logger, s\n",
    "read_config('./config.ini')\n",
    "\n",
    "'''\n",
    "initialize logging\n",
    "'''\n",
    "logger = logging.getLogger('SsLogger')\n",
    "logger.setLevel(config['log_level'])\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler = logging.handlers.RotatingFileHandler(config['log_file'], maxBytes=config['log_maxBytes'], backupCount=config['log_backupCount'])\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "'''\n",
    "read the IPs/CIDRs to be blacklisted\n",
    "'''\n",
    "blacklisted_ips_cidrs = read_blacklisted_ips()\n",
    "if len(blacklisted_ips_cidrs) > (config['max_ips_per_rule']*config['max_rules_per_policy']):\n",
    "    logger.error('Error: too many IPs/CIDRs ({}). Maximum limit: {}'.format(len(blacklisted_ips_cidrs),config['max_ips_per_rule']*config['max_rules_per_policy']))\n",
    "    exit_with_error()\n",
    "\n",
    "'''\n",
    "initiate session and authenticate\n",
    "'''\n",
    "logger.info('initiating session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OfficeProjVirEnv",
   "language": "python",
   "name": "officeprojvirenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
