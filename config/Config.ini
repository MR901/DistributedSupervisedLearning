# ||||||||||||||||||||||||||||||||||||||||||||||||||  START  |||||||||||||||||||||||||||||||||||||||||||||||||| #

# -------------------------------------------------------------------------------------------------------- #
#        <<<<[[[[ ADDITIONAL CUSTOMIZED MODULE SPECIFIC CONFIGURATION ]]]>>>>
# -------------------------------------------------------------------------------------------------------- #
[Config]
ModuleSettingRuleName = ICLSSTA
ICLSSTA_BinSizeBasedOnPeriod_Hr = 12
## Description: Size of the grouping methodology, another way to put this is that the traffic will be grouped based on the timestamps, and here this value represent those group size/ period size 


# -------------------------------------------------------------------------------------------------------- #
#        <<<<[[[[ ENVIRONMENT ]]]>>>>
# -------------------------------------------------------------------------------------------------------- #
[DomainConfig]
SIDs = ['xxxx']

BQ_GetNewCopyOfData = nooooooooTrue
## Description: Get new data or reuse the one that is already present
UseStaticOrDynamicCurrentDay = dynamic
## Options: 
##         'dynamic': use current_day as the actual current time
##         'static': use current_day as the one which is feeded
[IfStatic]
Date = ['160418', '170418', '180418']
DataGrabWindow_Days = 6
## 'Date' and 'DataToGrab_Days' can be used in combination
## if 'DataToGrab_Days' == '-': only 'Date' information is used
##                        else: first element of 'Date' is used and data for # of days provided by 'DataToGrab_Days' will extracted.
[IfDynamic]
DataGrabWindow_Hr = 12
## for past time i.e. to grab data between current to THIS many hours back ## Will Only affect, tasks = 'TrainTest'


[BigQueryConfig]
ProjectID = xxxxxxxxxxxxxx
## xxxxxxxxxxxxxx
BQ_LimitToStart = 10000000
## Description: no. of rows to extract if possible in single iteration
BQ_LimitDecreaseFactor = 1.25
## Geometric Progression (r)


# -------------------------------------------------------------------------------------------------------- #
#        <<<<[[[[ TASK ]]]>>>>
# -------------------------------------------------------------------------------------------------------- #
[IterationAim]
Task = TrainTest
## Options: 
##        'TrainTest': for Training as well as Test in which Train is used for Training and Rest is as Test 
##        'GlTest': GlTest uses data which is equal to 1hr+bin_0 
### Remove This ====> & if only one is provided then it automatically take split ratio to 100:0/0:100

GlTest_DataGrabWindow_Hr = 18
## Description: This can be same as Dynamic Grab Window_inhr

;TrainTest_SplitRatio = 70:30
;DevelopOutlierCluster = True
;DevelopConceptualDriftCluster = True

;CombineTrainTestAndGlTestData = False
## This is use to generate Result only with GlTest

;PaceMode = On
## options: On, Off ---> if Off Generate More Reports and Graphs






# -------------------------------------------------------------------------------------------------------- #
#        <<<<[[[[ DATA SCIENCE ]]]>>>>
# -------------------------------------------------------------------------------------------------------- #

# =========================== <<<<[[[[  DATA HANDLING  ]]]>>>> =========================== #
[DataCollection]
GetDataFrom = Storage
## Options : 
##         'BQ': Get Data from BQ
##         'Storage': load a csv from local storage

 

[DataProcessing_General]
FeaturesProcessing = { 'SID': {'Usage': 'Identification', 'DataType': 'Categorical', 'DataScaling': 'Nil', 'HandlingNAs': 'Ignore'}, 'BinsBackFromCurrent': {'Usage': 'Identification', 'DataType': 'Categorical', 'DataScaling': 'Nil', 'HandlingNAs': 'Ignore'}, 'apidata__zpsbd6': {'Usage': 'Identification', 'DataType': 'Categorical', 'DataScaling': 'Nil', 'HandlingNAs': 'Ignore'}, 'RecentHit_TimeStamp': {'Usage': 'Identification', 'DataType': 'Numeric', 'DataScaling': 'Nil', 'HandlingNAs': 'Ignore'}, 'isBotHits': {'Usage': 'RecentHit_TimeStamp', 'DataType': 'Numeric', 'DataScaling': 'Nil', 'HandlingNAs': 'Ignore'}, 'Hits': {'Usage': 'Identification', 'DataType': 'Numeric', 'DataScaling': 'Nil', 'HandlingNAs': 'Ignore'}, 'D_UzmaToD_UA': {'Usage': 'Analysis', 'DataType': 'Categorical', 'DataScaling': 'Standard', 'HandlingNAs': 'Ignore'}, 'HitsToD_Uzmc': {'Usage': 'Analysis', 'DataType': 'Categorical', 'DataScaling': 'Standard', 'HandlingNAs': 'Ignore'}, 'D_PageVisitedToHits': {'Usage': 'Analysis', 'DataType': 'Categorical', 'DataScaling': 'Standard', 'HandlingNAs': 'Ignore'}, 'PageActToD_PageVisit': {'Usage': 'Analysis', 'DataType': 'Categorical', 'DataScaling': 'Standard', 'HandlingNAs': 'Ignore'}, 'BrowsrActToD_BrowsrUsed': {'Usage': 'Analysis', 'DataType': 'Categorical', 'DataScaling': 'Standard', 'HandlingNAs': 'Ignore'}, 'AvgMedianTimeDiffBWHits': {'Usage': 'Analysis', 'DataType': 'Categorical', 'DataScaling': 'Standard', 'HandlingNAs': 'Ignore'}, 'StandDeviatAvgTimeDiffBWHits': {'Usage': 'Analysis', 'DataType': 'Categorical', 'DataScaling': 'Standard', 'HandlingNAs': 'Ignore'}, 'AvgHitsPerUnitTime': {'Usage': 'Analysis', 'DataType': 'Categorical', 'DataScaling': 'Standard', 'HandlingNAs': 'Ignore'}, 'DiffOfAvgTimeDiffBWHitsWhnGrpIPAndIPUzma': {'Usage': 'Analysis', 'DataType': 'Categorical', 'DataScaling': 'Standard', 'HandlingNAs': 'Ignore'} }
## Options:
##        'Usage': 'Identification' // 'Analysis'
##        'DataType': 'Categorical' // 'Numeric'
##        'DataScaling': 'OneHotEncoding' // 'DummyEncoding' // 'Standard' // 'Standard_Median' // 'Normalized' // 'Nil'
##        'HandlingNAs': 'DropRows' // 'FillMostFreq' // 'FillAverage' // 'FillMedian' // 'Ignore'
## By Default: undefined feature will be dropped from 'Identifation' and 'Analysis'

KeyFormat = ['SID', 'BinsBackFromCurrent', 'apidata__zpsbd6']
## Decription: used to Create Unique Identification by joining the value accross multiple columns mentioned above, original columns are dropped.

TreatKeyAsUniqueIdentifaction = True
## Description: If True will mean that each keys needs to be unique and when true, All Duplicate observation will be dropped.






# ------------<<<< ALGORITHM SETTING CONFIGURATION >>>>------------ #
# ---------------------<<<< FEEDBACK DATA >>>>--------------------- #

# =========================== <<<<[[[[  ACTION TAKING  ]]]>>>> =========================== #





# -------------------------------------------------------------------------------------------------------- #
#        <<<<[[[[ PATHS ]]]>>>>
# -------------------------------------------------------------------------------------------------------- #

[InputPaths]
BQ_DataImportQuery = ../config/Query_DataImport_Supervised.txt
BQ_RawDataStoringName = ../data/InputData/RawExtractedData_{}.csv
Storage_RawData = ../data/InputData/RawExtractedData_TrainTest.csv
#RawInputData.csv

[TempPaths]


[ModelPaths]

[OutputPaths]


[LogPaths]
ExecutionTimeTaken = ../logs/TimeConsumed.csv
RecommendationFile = ../logs/Recommendation.csv







# ||||||||||||||||||||||||||||||||||||||||||||||||||  END  |||||||||||||||||||||||||||||||||||||||||||||||||| #
